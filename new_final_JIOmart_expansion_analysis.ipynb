{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JioMart Tier 2/3 Cities Expansion Analysis\n",
    "## End-to-End Data Science Portfolio Project\n",
    "\n",
    "---\n",
    "\n",
    "### Business Context\n",
    "**JioMart**, the digital commerce arm of Reliance Retail Ventures Ltd., is aggressively scaling operations into non-metro Tier 2 and Tier 3 cities across India, expanding to 5,000+ pin codes and 3,000+ stores.\n",
    "\n",
    "### Problem Statement\n",
    "How can JioMart optimize its expansion strategy into Tier 2/3 cities to improve profitability, reduce logistics and inventory costs, and enhance customer retention?\n",
    "\n",
    "### Hypothesis\n",
    "Margin erosion and lower repeat purchase rates in Tier 2/3 cities are driven by:\n",
    "1. Higher logistics and last-mile costs\n",
    "2. Product assortment mismatch\n",
    "3. Weaker customer loyalty\n",
    "4. Insufficient infrastructure leading to spoilage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, silhouette_score, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load the pre-generated datasets from the complete analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transactions_df = pd.read_csv('data/transactions.csv')\n",
    "customers_df = pd.read_csv('data/customers.csv')\n",
    "stores_df = pd.read_csv('data/stores.csv')\n",
    "products_df = pd.read_csv('data/products.csv')\n",
    "inventory_df = pd.read_csv('data/inventory.csv')\n",
    "\n",
    "print(\"✅ Data loaded successfully\\n\")\n",
    "print(f\"Transactions: {len(transactions_df):,} records\")\n",
    "print(f\"Customers: {len(customers_df):,} records\")\n",
    "print(f\"Stores: {len(stores_df):,} records\")\n",
    "print(f\"Products: {len(products_df):,} records\")\n",
    "print(f\"Inventory: {len(inventory_df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data overview\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSACTION DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "transactions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"\\nData Info:\")\n",
    "print(transactions_df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(transactions_df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "transactions_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Regional Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional performance summary\n",
    "regional_perf = transactions_df.groupby('region_tier').agg({\n",
    "    'transaction_id': 'count',\n",
    "    'revenue': 'sum',\n",
    "    'margin': 'sum',\n",
    "    'customer_id': 'nunique',\n",
    "    'delivery_time_hours': 'mean',\n",
    "    'delivery_distance_km': 'mean',\n",
    "    'logistics_cost': 'mean',\n",
    "    'spoilage_cost': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "regional_perf.columns = ['Transactions', 'Total Revenue (₹)', 'Total Margin (₹)', 'Unique Customers', \n",
    "                          'Avg Delivery Time (hrs)', 'Avg Delivery Distance (km)', \n",
    "                          'Avg Logistics Cost (₹)', 'Avg Spoilage Cost (₹)']\n",
    "regional_perf['Margin %'] = ((regional_perf['Total Margin (₹)'] / regional_perf['Total Revenue (₹)']) * 100).round(2)\n",
    "regional_perf['Revenue per Customer (₹)'] = (regional_perf['Total Revenue (₹)'] / regional_perf['Unique Customers']).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGIONAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "regional_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualization: Regional Performance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Revenue by region\n",
    "revenue_data = regional_perf['Total Revenue (₹)'] / 1_000_000\n",
    "axes[0, 0].bar(revenue_data.index, revenue_data.values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 0].set_title('Total Revenue by Region (₹ Millions)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Revenue (₹M)')\n",
    "for i, v in enumerate(revenue_data.values):\n",
    "    axes[0, 0].text(i, v + 1, f'₹{v:.1f}M', ha='center', fontweight='bold')\n",
    "\n",
    "# Margin %\n",
    "margin_data = regional_perf['Margin %']\n",
    "axes[0, 1].bar(margin_data.index, margin_data.values, color=['#d62728', '#9467bd', '#8c564b'])\n",
    "axes[0, 1].set_title('Profit Margin % by Region', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Margin %')\n",
    "axes[0, 1].axhline(y=margin_data.mean(), color='red', linestyle='--', label='Average')\n",
    "axes[0, 1].legend()\n",
    "for i, v in enumerate(margin_data.values):\n",
    "    axes[0, 1].text(i, v + 0.5, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Delivery Time\n",
    "delivery_data = regional_perf['Avg Delivery Time (hrs)']\n",
    "axes[1, 0].bar(delivery_data.index, delivery_data.values, color=['#e377c2', '#7f7f7f', '#bcbd22'])\n",
    "axes[1, 0].set_title('Avg Delivery Time by Region', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Hours')\n",
    "for i, v in enumerate(delivery_data.values):\n",
    "    axes[1, 0].text(i, v + 0.3, f'{v:.1f}h', ha='center', fontweight='bold')\n",
    "\n",
    "# Logistics Cost\n",
    "logistics_data = regional_perf['Avg Logistics Cost (₹)']\n",
    "axes[1, 1].bar(logistics_data.index, logistics_data.values, color=['#17becf', '#1f77b4', '#ff7f0e'])\n",
    "axes[1, 1].set_title('Avg Logistics Cost by Region', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Cost (₹)')\n",
    "for i, v in enumerate(logistics_data.values):\n",
    "    axes[1, 1].text(i, v + 3, f'₹{v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Customer Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer purchase patterns\n",
    "customer_behavior = transactions_df.groupby(['customer_id', 'region_tier']).agg({\n",
    "    'transaction_id': 'count',\n",
    "    'revenue': 'sum',\n",
    "    'margin': 'sum'\n",
    "}).reset_index()\n",
    "customer_behavior.columns = ['customer_id', 'region_tier', 'purchase_count', 'total_revenue', 'total_margin']\n",
    "\n",
    "behavior_summary = customer_behavior.groupby('region_tier').agg({\n",
    "    'purchase_count': 'mean',\n",
    "    'total_revenue': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "behavior_summary.columns = ['Avg Purchases per Customer', 'Avg Revenue per Customer (₹)', 'Total Customers']\n",
    "\n",
    "# Repeat purchase rate (3+ purchases)\n",
    "repeat_customers = customer_behavior[customer_behavior['purchase_count'] >= 3].groupby('region_tier').size()\n",
    "total_customers = customer_behavior.groupby('region_tier').size()\n",
    "behavior_summary['Repeat Rate (%)'] = ((repeat_customers / total_customers) * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CUSTOMER BEHAVIOR SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "behavior_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize customer behavior\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Purchase frequency distribution\n",
    "for tier in ['Metro', 'Tier 2', 'Tier 3']:\n",
    "    tier_cust = customer_behavior[customer_behavior['region_tier'] == tier]['purchase_count']\n",
    "    axes[0].hist(tier_cust, bins=20, alpha=0.6, label=tier)\n",
    "axes[0].set_title('Purchase Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Purchases')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 20)\n",
    "\n",
    "# Repeat rate by region\n",
    "repeat_rate_data = behavior_summary['Repeat Rate (%)']\n",
    "axes[1].bar(repeat_rate_data.index, repeat_rate_data.values, color=['#2ca02c', '#ff7f0e', '#d62728'])\n",
    "axes[1].set_title('Customer Repeat Purchase Rate (3+ orders)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Repeat Rate (%)')\n",
    "for i, v in enumerate(repeat_rate_data.values):\n",
    "    axes[1].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Category Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with product data\n",
    "txn_with_product = transactions_df.merge(products_df[['product_id', 'category', 'product_name']], on='product_id')\n",
    "\n",
    "# Category performance by region\n",
    "category_perf = txn_with_product.groupby(['region_tier', 'category']).agg({\n",
    "    'revenue': 'sum',\n",
    "    'margin': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "category_perf['margin_pct'] = ((category_perf['margin'] / category_perf['revenue']) * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP CATEGORIES BY REGION\")\n",
    "print(\"=\" * 80)\n",
    "category_perf.sort_values(['region_tier', 'revenue'], ascending=[True, False]).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Category revenue by tier\n",
    "category_revenue = txn_with_product.pivot_table(values='revenue', index='category', columns='region_tier', aggfunc='sum') / 1000\n",
    "category_revenue.plot(kind='barh', ax=axes[0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_title('Revenue by Category & Region (₹ Thousands)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Revenue (₹K)')\n",
    "axes[0].legend(title='Region')\n",
    "\n",
    "# Category margin %\n",
    "category_margins = category_perf.pivot_table(values='margin_pct', index='category', columns='region_tier')\n",
    "category_margins.plot(kind='bar', ax=axes[1], color=['#d62728', '#9467bd', '#8c564b'])\n",
    "axes[1].set_title('Margin % by Category & Region', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Margin %')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend(title='Region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Logistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost breakdown by region\n",
    "logistics_breakdown = transactions_df.groupby('region_tier').agg({\n",
    "    'product_cost': 'mean',\n",
    "    'logistics_cost': 'mean',\n",
    "    'spoilage_cost': 'mean',\n",
    "    'total_cost': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COST BREAKDOWN BY REGION\")\n",
    "print(\"=\" * 80)\n",
    "logistics_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize logistics analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cost breakdown\n",
    "logistics_breakdown[['product_cost', 'logistics_cost', 'spoilage_cost']].plot(\n",
    "    kind='bar', stacked=True, ax=axes[0], color=['#8c564b', '#e377c2', '#d62728']\n",
    ")\n",
    "axes[0].set_title('Cost Breakdown by Region', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Avg Cost (₹)')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "axes[0].legend(['Product Cost', 'Logistics Cost', 'Spoilage Cost'])\n",
    "\n",
    "# Delivery time vs distance scatter\n",
    "for tier, color in [('Metro', 'blue'), ('Tier 2', 'orange'), ('Tier 3', 'green')]:\n",
    "    data = transactions_df[transactions_df['region_tier'] == tier].sample(min(1000, len(transactions_df[transactions_df['region_tier'] == tier])))\n",
    "    axes[1].scatter(data['delivery_distance_km'], data['delivery_time_hours'], alpha=0.3, s=10, label=tier, color=color)\n",
    "axes[1].set_title('Delivery Time vs Distance', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Distance (km)')\n",
    "axes[1].set_ylabel('Time (hours)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Models\n",
    "\n",
    "### 4.1 Model 1: Margin Risk Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: MARGIN RISK CLASSIFICATION (Random Forest)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aggregate store performance\n",
    "store_perf = transactions_df.groupby('store_id').agg({\n",
    "    'revenue': 'sum',\n",
    "    'margin': 'sum',\n",
    "    'margin_pct': 'mean',\n",
    "    'logistics_cost': 'mean',\n",
    "    'spoilage_cost': 'mean',\n",
    "    'delivery_time_hours': 'mean',\n",
    "    'is_perishable': 'mean',\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "store_perf = store_perf.merge(stores_df[['store_id', 'region_tier', 'infrastructure_score', 'warehouse_distance_km']], on='store_id')\n",
    "\n",
    "# Define high risk: margin % < 10%\n",
    "store_perf['high_risk'] = (store_perf['margin_pct'] < 10).astype(int)\n",
    "\n",
    "print(f\"\\nHigh Risk Stores: {store_perf['high_risk'].sum()} / {len(store_perf)}\")\n",
    "print(f\"Low Risk Stores: {(1 - store_perf['high_risk']).sum()} / {len(store_perf)}\")\n",
    "\n",
    "# Prepare features\n",
    "le = LabelEncoder()\n",
    "store_perf['region_code'] = le.fit_transform(store_perf['region_tier'])\n",
    "\n",
    "X = store_perf[['revenue', 'logistics_cost', 'spoilage_cost', 'delivery_time_hours', \n",
    "                'is_perishable', 'infrastructure_score', 'warehouse_distance_km', \n",
    "                'transaction_id', 'region_code']]\n",
    "y = store_perf['high_risk']\n",
    "\n",
    "# Train-test split\n",
    "if len(y.unique()) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Feature Importances:\")\n",
    "    print(feature_importance.head())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'][:8], feature_importance['importance'][:8])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top Features for Margin Risk Prediction', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n⚠ Insufficient class diversity for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model 2: Customer Lifetime Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 2: CUSTOMER LIFETIME VALUE PREDICTION (Gradient Boosting)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare customer features\n",
    "cust_features = customer_behavior.merge(customers_df[['customer_id', 'age', 'income_bracket', \n",
    "                                                       'digital_literacy_score', 'registration_date']], \n",
    "                                        on='customer_id')\n",
    "cust_features['registration_date'] = pd.to_datetime(cust_features['registration_date'])\n",
    "cust_features['days_since_registration'] = (datetime(2024, 9, 30) - cust_features['registration_date']).dt.days\n",
    "\n",
    "# Encode income bracket\n",
    "income_mapping = {'10-15K': 1, '15-25K': 2, '25-50K': 3, '50-75K': 4, '75K+': 5}\n",
    "cust_features['income_code'] = cust_features['income_bracket'].map(income_mapping)\n",
    "cust_features['income_code'] = cust_features['income_code'].fillna(2)  # Default to mid-range\n",
    "\n",
    "# Features and target\n",
    "X_clv = cust_features[['purchase_count', 'age', 'income_code', 'digital_literacy_score', 'days_since_registration']]\n",
    "X_clv['region_code'] = le.transform(cust_features['region_tier'])\n",
    "y_clv = cust_features['total_revenue']\n",
    "\n",
    "# Remove any NaN values\n",
    "mask = ~(X_clv.isna().any(axis=1) | y_clv.isna())\n",
    "X_clv = X_clv[mask]\n",
    "y_clv = y_clv[mask]\n",
    "\n",
    "# Train-test split\n",
    "X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(X_clv, y_clv, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler_clv = StandardScaler()\n",
    "X_train_clv_scaled = scaler_clv.fit_transform(X_train_clv)\n",
    "X_test_clv_scaled = scaler_clv.transform(X_test_clv)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train_clv_scaled, y_train_clv)\n",
    "\n",
    "y_pred_clv = gb_model.predict(X_test_clv_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test_clv, y_pred_clv)\n",
    "r2 = r2_score(y_test_clv, y_pred_clv)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  MAE: ₹{mae:.2f}\")\n",
    "print(f\"  R² Score: {r2:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_clv, y_pred_clv, alpha=0.5, s=20)\n",
    "plt.plot([y_test_clv.min(), y_test_clv.max()], [y_test_clv.min(), y_test_clv.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Revenue (₹)')\n",
    "plt.ylabel('Predicted Revenue (₹)')\n",
    "plt.title('Customer Lifetime Value: Predicted vs Actual', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model 3: Customer Segmentation (K-Means Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 3: CUSTOMER SEGMENTATION (K-Means Clustering)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare clustering features\n",
    "cluster_data = cust_features[['purchase_count', 'total_revenue', 'total_margin', \n",
    "                               'age', 'income_code', 'digital_literacy_score']].copy()\n",
    "\n",
    "# Remove NaN\n",
    "cluster_data = cluster_data.dropna()\n",
    "\n",
    "# Scale for clustering\n",
    "scaler_cluster = StandardScaler()\n",
    "cluster_scaled = scaler_cluster.fit_transform(cluster_data)\n",
    "\n",
    "# Find optimal k using silhouette score\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(cluster_scaled)\n",
    "    silhouette_scores.append(silhouette_score(cluster_scaled, labels))\n",
    "\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.4f}\")\n",
    "\n",
    "# Train final model\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(cluster_scaled)\n",
    "\n",
    "# Add cluster labels back to original data\n",
    "cluster_data_with_labels = cluster_data.copy()\n",
    "cluster_data_with_labels['cluster'] = cluster_labels\n",
    "\n",
    "# Cluster profiles\n",
    "cluster_profiles = cluster_data_with_labels.groupby('cluster').agg({\n",
    "    'purchase_count': ['count', 'mean'],\n",
    "    'total_revenue': 'mean',\n",
    "    'total_margin': 'mean',\n",
    "    'age': 'mean',\n",
    "    'income_code': 'mean',\n",
    "    'digital_literacy_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cluster distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "axes[0].pie(cluster_counts.values, labels=[f'Cluster {i}' for i in cluster_counts.index], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Customer Segment Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Revenue by cluster\n",
    "cluster_revenue = cluster_data_with_labels.groupby('cluster')['total_revenue'].sum() / 1000\n",
    "axes[1].bar(cluster_revenue.index, cluster_revenue.values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "axes[1].set_title('Total Revenue by Cluster (₹ Thousands)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Revenue (₹K)')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Insights & Recommendations\n",
    "\n",
    "### 5.1 Critical Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate key insights\n",
    "metro_margin = regional_perf.loc['Metro', 'Margin %']\n",
    "tier3_margin = regional_perf.loc['Tier 3', 'Margin %']\n",
    "margin_gap = metro_margin - tier3_margin\n",
    "\n",
    "metro_logistics = regional_perf.loc['Metro', 'Avg Logistics Cost (₹)']\n",
    "tier3_logistics = regional_perf.loc['Tier 3', 'Avg Logistics Cost (₹)']\n",
    "logistics_increase = ((tier3_logistics - metro_logistics) / metro_logistics) * 100\n",
    "\n",
    "metro_delivery = regional_perf.loc['Metro', 'Avg Delivery Time (hrs)']\n",
    "tier3_delivery = regional_perf.loc['Tier 3', 'Avg Delivery Time (hrs)']\n",
    "time_increase = tier3_delivery - metro_delivery\n",
    "\n",
    "tier2_customers = regional_perf.loc['Tier 2', 'Unique Customers']\n",
    "tier3_customers = regional_perf.loc['Tier 3', 'Unique Customers']\n",
    "total_untapped = tier2_customers + tier3_customers\n",
    "\n",
    "print(f\"\\n1. 📊 MARGIN GAP:\")\n",
    "print(f\"   Tier 3 cities have {margin_gap:.1f}% lower margins than Metro\")\n",
    "print(f\"   Metro: {metro_margin:.2f}% vs Tier 3: {tier3_margin:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. 🚚 LOGISTICS CHALLENGE:\")\n",
    "print(f\"   Tier 3 logistics costs are {logistics_increase:.0f}% higher than Metro\")\n",
    "print(f\"   Metro: ₹{metro_logistics:.2f} vs Tier 3: ₹{tier3_logistics:.2f}\")\n",
    "\n",
    "print(f\"\\n3. ⏱️ DELIVERY DELAY:\")\n",
    "print(f\"   Tier 3 deliveries take {time_increase:.1f} hours longer than Metro\")\n",
    "print(f\"   Metro: {metro_delivery:.1f}h vs Tier 3: {tier3_delivery:.1f}h\")\n",
    "\n",
    "print(f\"\\n4. 💰 GROWTH POTENTIAL:\")\n",
    "print(f\"   {total_untapped:,.0f} customers in Tier 2/3 represent untapped revenue\")\n",
    "print(f\"   Tier 2: {tier2_customers:,.0f} | Tier 3: {tier3_customers:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = {\n",
    "    \"1. Optimize Last-Mile Logistics\": [\n",
    "        \"• Establish micro-fulfillment centers in Tier 2/3 city clusters\",\n",
    "        \"• Partner with local logistics providers familiar with regional terrain\",\n",
    "        \"• Implement hub-and-spoke distribution model\"\n",
    "    ],\n",
    "    \"2. Improve Inventory Management\": [\n",
    "        \"• Deploy predictive analytics for demand forecasting\",\n",
    "        \"• Reduce perishable inventory in Tier 3 stores initially\",\n",
    "        \"• Implement FIFO strictly for fresh produce\"\n",
    "    ],\n",
    "    \"3. Tailor Product Assortment\": [\n",
    "        \"• Focus on non-perishables for Tier 3 (Groceries, Home Care)\",\n",
    "        \"• Gradually introduce premium products based on digital literacy\",\n",
    "        \"• Create region-specific bundles\"\n",
    "    ],\n",
    "    \"4. Enhance Customer Retention\": [\n",
    "        \"• Launch loyalty programs with tier-based rewards\",\n",
    "        \"• Offer free delivery for repeat customers in Tier 2/3\",\n",
    "        \"• Use targeted WhatsApp marketing\"\n",
    "    ],\n",
    "    \"5. Technology Deployment\": [\n",
    "        \"• Deploy route optimization software\",\n",
    "        \"• Implement IoT sensors for cold chain monitoring\",\n",
    "        \"• Use AI chatbots for customer support\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for key, actions in recommendations.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    for action in actions:\n",
    "        print(f\"  {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Key Metrics:\")\n",
    "print(f\"  • Total Transactions: {len(transactions_df):,}\")\n",
    "print(f\"  • Total Revenue: ₹{transactions_df['revenue'].sum()/1_000_000:.2f}M\")\n",
    "print(f\"  • Total Margin: ₹{transactions_df['margin'].sum()/1_000_000:.2f}M\")\n",
    "print(f\"  • Unique Customers: {transactions_df['customer_id'].nunique():,}\")\n",
    "print(f\"  • Active Stores: {len(stores_df)}\")\n",
    "print(f\"  • Product SKUs: {len(products_df)}\")\n",
    "\n",
    "print(f\"\\n🎯 Models Trained:\")\n",
    "print(f\"  1. Random Forest - Margin Risk Classification\")\n",
    "print(f\"  2. Gradient Boosting - Customer Lifetime Value Prediction\")\n",
    "print(f\"  3. K-Means Clustering - Customer Segmentation\")\n",
    "\n",
    "print(f\"\\n✅ Analysis Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Analysis\n",
    "\n",
    "**Project:** JioMart Tier 2/3 Cities Expansion Analysis  \n",
    "**Author:** Data Science Portfolio  \n",
    "**Date:** October 2024\n",
    "\n",
    "This notebook demonstrates end-to-end data science capabilities including:\n",
    "- Data analysis and exploration\n",
    "- Statistical insights and business intelligence\n",
    "- Machine learning model development\n",
    "- Visualization and storytelling\n",
    "- Strategic recommendation generation\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
